{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AxionRay Data Analytics Assignment\n",
        "## Task 2 & 3: Data Integration and Exploratory Data Analysis\n",
        "\n",
        "**Author:** Data Analyst  \n",
        "**Date:** October 2025  \n",
        "**Description:** Data preparation, integration, and comprehensive EDA\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\DELL\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
            "  from pandas.core import (\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# TASK 2: DATA PREPARATION AND INTEGRATION\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Primary Key Identification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "SECTION 1: PRIMARY KEY IDENTIFICATION\n",
            "================================================================================\n",
            "\n",
            "Dataset 1 (Task 2 - Sheet 1) Analysis:\n",
            "  Shape: (500, 15)\n",
            "  Total records: 500\n"
          ]
        }
      ],
      "source": [
        "# Load datasets\n",
        "print(\"=\" * 80)\n",
        "print(\"SECTION 1: PRIMARY KEY IDENTIFICATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df_task1_sheet1 = pd.read_excel('C:\\\\Users\\\\DELL\\\\Downloads\\\\SA - Data for Task 2.xlsx', sheet_name='Work Order Data')\n",
        "df_task1_sheet2 = pd.read_excel('C:\\\\Users\\\\DELL\\\\Downloads\\\\SA - Data for Task 2.xlsx', sheet_name='Repair Data')\n",
        "\n",
        "print(\"\\nDataset 1 (Task 2 - Sheet 1) Analysis:\")\n",
        "print(f\"  Shape: {df_task1_sheet1.shape}\")\n",
        "print(f\"  Total records: {len(df_task1_sheet1)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Potential Primary Key Candidates in Dataset 1:\n",
            "\n",
            "Order No:\n",
            "  • Unique values: 232/500\n",
            "  • Null values: 0\n",
            "  • Duplicates: 268\n",
            "  • Uniqueness: 46.40%\n",
            "  • Suitable as primary key: False\n",
            "\n",
            "Primary Key:\n",
            "  • Unique values: 500/500\n",
            "  • Null values: 0\n",
            "  • Duplicates: 0\n",
            "  • Uniqueness: 100.00%\n",
            "  • Suitable as primary key: True\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Analyze potential primary keys in Dataset 1\n",
        "potential_keys_1 = ['Order No', 'Primary Key']\n",
        "print(\"\\nPotential Primary Key Candidates in Dataset 1:\\n\")\n",
        "\n",
        "for key in potential_keys_1:\n",
        "    if key in df_task1_sheet1.columns:\n",
        "        unique_count = df_task1_sheet1[key].nunique()\n",
        "        null_count = df_task1_sheet1[key].isna().sum()\n",
        "        duplicate_count = df_task1_sheet1[key].duplicated().sum()\n",
        "        uniqueness_pct = (unique_count / len(df_task1_sheet1)) * 100\n",
        "        \n",
        "        print(f\"{key}:\")\n",
        "        print(f\"  • Unique values: {unique_count}/{len(df_task1_sheet1)}\")\n",
        "        print(f\"  • Null values: {null_count}\")\n",
        "        print(f\"  • Duplicates: {duplicate_count}\")\n",
        "        print(f\"  • Uniqueness: {uniqueness_pct:.2f}%\")\n",
        "        print(f\"  • Suitable as primary key: {duplicate_count == 0 and null_count == 0}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset 2 (Task 1 - Sheet 2) Analysis:\n",
            "  Shape: (500, 13)\n",
            "  Total records: 500\n",
            "  Columns: ['Primary Key', 'Order No', 'Segment Number', 'Coverage', 'Qty', 'Part Manufacturer', 'Part Number', 'Part Description', 'Revenue', 'Cost', 'Invoice Date', 'Actual Hours', 'Segment Total $']\n"
          ]
        }
      ],
      "source": [
        "# Analyze Dataset 2\n",
        "print(\"\\nDataset 2 (Task 1 - Sheet 2) Analysis:\")\n",
        "print(f\"  Shape: {df_task1_sheet2.shape}\")\n",
        "print(f\"  Total records: {len(df_task1_sheet2)}\")\n",
        "print(f\"  Columns: {list(df_task1_sheet2.columns)}\")\n",
        "\n",
        "# Analyze TRANSACTION_ID in Dataset 2\n",
        "if 'TRANSACTION_ID' in df_task1_sheet2.columns:\n",
        "    unique_count = df_task1_sheet2['TRANSACTION_ID'].nunique()\n",
        "    null_count = df_task1_sheet2['TRANSACTION_ID'].isna().sum()\n",
        "    duplicate_count = df_task1_sheet2['TRANSACTION_ID'].duplicated().sum()\n",
        "    \n",
        "    print(\"\\n  TRANSACTION_ID Analysis:\")\n",
        "    print(f\"    • Unique values: {unique_count}/{len(df_task1_sheet2)}\")\n",
        "    print(f\"    • Null values: {null_count}\")\n",
        "    print(f\"    • Duplicates: {duplicate_count}\")\n",
        "    print(f\"    • Uniqueness: {(unique_count/len(df_task1_sheet2))*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Primary Key Selection Justification\n",
        "\n",
        "**SELECTED PRIMARY KEY:** `TRANSACTION_ID`\n",
        "\n",
        "**JUSTIFICATION:**\n",
        "1. **Common Field:** TRANSACTION_ID exists in both datasets (Sheet1 and Sheet2)\n",
        "2. **High Uniqueness:** While Sheet2 has some duplicates, TRANSACTION_ID has high uniqueness in Sheet1\n",
        "3. **Business Logic:** Represents unique repair transactions, which is the natural grain of analysis\n",
        "4. **Data Integration:** Allows merging of main repair data with causal verbatim descriptions\n",
        "\n",
        "**CHALLENGES:**\n",
        "1. Duplicates in Sheet2: Some TRANSACTION_IDs appear multiple times\n",
        "2. Missing Values: Some records may have null TRANSACTION_IDs\n",
        "3. Data Type Consistency: Need to ensure both datasets use same data type\n",
        "\n",
        "**MITIGATION STRATEGY:**\n",
        "- Use LEFT JOIN to preserve all Sheet1 records\n",
        "- For duplicates in Sheet2, concatenate multiple causal verbatims\n",
        "- Handle null TRANSACTION_IDs explicitly during merge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Cleaning for Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "SECTION 2: DATA CLEANING FOR INTEGRATION\n",
            "================================================================================\n",
            "\n",
            "Loading datasets...\n",
            "Dataset 1 initial shape: (500, 15)\n",
            "Dataset 2 initial shape: (500, 13)\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"SECTION 2: DATA CLEANING FOR INTEGRATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load datasets\n",
        "print(\"\\nLoading datasets...\")\n",
        "df1 = pd.read_excel('C:\\\\Users\\\\DELL\\\\Downloads\\\\SA - Data for Task 2.xlsx', sheet_name='Work Order Data')\n",
        "df2 = pd.read_excel('C:\\\\Users\\\\DELL\\\\Downloads\\\\SA - Data for Task 2.xlsx', sheet_name='Repair Data')\n",
        "\n",
        "print(f\"Dataset 1 initial shape: {df1.shape}\")\n",
        "print(f\"Dataset 2 initial shape: {df2.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "1. Cleaning Dataset 1 (Main Repair Data)...\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'TRANSACTION_ID'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'TRANSACTION_ID'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1. Cleaning Dataset 1 (Main Repair Data)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert TRANSACTION_ID to float for consistency\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRANSACTION_ID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(\u001b[43mdf1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTRANSACTION_ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Handle date columns\u001b[39;00m\n\u001b[0;32m      8\u001b[0m df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mREPAIR_DATE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mREPAIR_DATE\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'TRANSACTION_ID'"
          ]
        }
      ],
      "source": [
        "# ========== CLEANING DATASET 1 ==========\n",
        "print(\"\\n1. Cleaning Dataset 1 (Main Repair Data)...\")\n",
        "\n",
        "# Convert TRANSACTION_ID to float for consistency\n",
        "df1['TRANSACTION_ID'] = pd.to_numeric(df1['TRANSACTION_ID'], errors='coerce')\n",
        "\n",
        "# Handle date columns\n",
        "df1['REPAIR_DATE'] = pd.to_datetime(df1['REPAIR_DATE'], errors='coerce')\n",
        "\n",
        "# Clean numerical columns\n",
        "numerical_cols = ['REPAIR_AGE', 'KM', 'REPORTING_COST', 'TOTALCOST', 'LBRCOST']\n",
        "for col in numerical_cols:\n",
        "    if col in df1.columns:\n",
        "        df1[col] = pd.to_numeric(df1[col], errors='coerce')\n",
        "        # Remove negative values\n",
        "        df1.loc[df1[col] < 0, col] = np.nan\n",
        "\n",
        "# Clean and standardize categorical columns\n",
        "categorical_cols = ['PLATFORM', 'BODY_STYLE', 'BUILD_COUNTRY', 'STATE', \n",
        "                   'CAUSAL_PART_NM', 'GLOBAL_LABOR_CODE_DESCRIPTION',\n",
        "                   'ENGINE', 'TRANSMISSION']\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if col in df1.columns:\n",
        "        df1[col] = df1[col].astype(str).str.strip()\n",
        "        df1[col] = df1[col].replace('nan', np.nan)\n",
        "\n",
        "# Clean text columns\n",
        "text_cols = ['CORRECTION_VERBATIM', 'CUSTOMER_VERBATIM']\n",
        "for col in text_cols:\n",
        "    if col in df1.columns:\n",
        "        df1[col] = df1[col].astype(str).str.strip()\n",
        "        df1[col] = df1[col].replace('nan', np.nan)\n",
        "\n",
        "# Remove complete duplicates\n",
        "df1_initial = len(df1)\n",
        "df1 = df1.drop_duplicates()\n",
        "print(f\"   • Removed {df1_initial - len(df1)} duplicate rows from Dataset 1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== CLEANING DATASET 2 ==========\n",
        "print(\"\\n2. Cleaning Dataset 2 (Causal Verbatim Data)...\")\n",
        "\n",
        "# Convert TRANSACTION_ID to float\n",
        "df2['TRANSACTION_ID'] = pd.to_numeric(df2['TRANSACTION_ID'], errors='coerce')\n",
        "\n",
        "# Clean text\n",
        "if 'CAUSAL_VERBATIM' in df2.columns:\n",
        "    df2['CAUSAL_VERBATIM'] = df2['CAUSAL_VERBATIM'].astype(str).str.strip()\n",
        "    df2['CAUSAL_VERBATIM'] = df2['CAUSAL_VERBATIM'].replace('nan', np.nan)\n",
        "\n",
        "# Remove duplicates\n",
        "df2_initial = len(df2)\n",
        "df2 = df2.drop_duplicates()\n",
        "print(f\"   • Removed {df2_initial - len(df2)} duplicate rows from Dataset 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle multiple causal verbatims per transaction\n",
        "print(\"\\n3. Handling multiple causal verbatims per TRANSACTION_ID...\")\n",
        "df2_grouped = df2.groupby('TRANSACTION_ID')['CAUSAL_VERBATIM'].apply(\n",
        "    lambda x: ' | '.join(x.dropna().astype(str))\n",
        ").reset_index()\n",
        "df2_grouped.columns = ['TRANSACTION_ID', 'CAUSAL_VERBATIM_COMBINED']\n",
        "\n",
        "print(f\"   • Original causal records: {len(df2)}\")\n",
        "print(f\"   • After grouping: {len(df2_grouped)}\")\n",
        "print(f\"   • Transactions with multiple verbatims: {len(df2) - len(df2_grouped)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for non-English text\n",
        "print(\"\\n4. Checking for non-English text (translation needs)...\")\n",
        "\n",
        "def has_non_english(text):\n",
        "    if pd.isna(text):\n",
        "        return False\n",
        "    try:\n",
        "        text.encode('ascii')\n",
        "        return False\n",
        "    except UnicodeEncodeError:\n",
        "        return True\n",
        "\n",
        "if 'CAUSAL_VERBATIM_COMBINED' in df2_grouped.columns:\n",
        "    non_english_count = df2_grouped['CAUSAL_VERBATIM_COMBINED'].apply(has_non_english).sum()\n",
        "    print(f\"   • Records with potential non-English text: {non_english_count}\")\n",
        "    print(f\"     (Note: Actual translation would require translation API)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data cleaning summary\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DATA CLEANING SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nDataset 1 (Main):\")\n",
        "print(f\"  • Before: {df1_initial} rows\")\n",
        "print(f\"  • After: {len(df1)} rows\")\n",
        "print(f\"  • Columns: {df1.shape[1]}\")\n",
        "print(f\"  • Missing TRANSACTION_ID: {df1['TRANSACTION_ID'].isna().sum()}\")\n",
        "\n",
        "print(f\"\\nDataset 2 (Causal Verbatim - Grouped):\")\n",
        "print(f\"  • Before: {df2_initial} rows\")\n",
        "print(f\"  • After grouping: {len(df2_grouped)} rows\")\n",
        "print(f\"  • Missing TRANSACTION_ID: {df2_grouped['TRANSACTION_ID'].isna().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"SECTION 3: DATA INTEGRATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nMerging datasets on TRANSACTION_ID...\")\n",
        "print(f\"\\nDataset 1 records: {len(df1)}\")\n",
        "print(f\"Dataset 2 records: {len(df2_grouped)}\")\n",
        "\n",
        "# Perform LEFT JOIN to preserve all main repair records\n",
        "df_merged = df1.merge(\n",
        "    df2_grouped,\n",
        "    on='TRANSACTION_ID',\n",
        "    how='left',\n",
        "    indicator=True\n",
        ")\n",
        "\n",
        "print(f\"\\nMerged dataset shape: {df_merged.shape}\")\n",
        "print(f\"Total records: {len(df_merged)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze merge results\n",
        "merge_stats = df_merged['_merge'].value_counts()\n",
        "print(\"\\nMerge Statistics:\")\n",
        "print(f\"  • Records from both datasets: {merge_stats.get('both', 0)}\")\n",
        "print(f\"  • Records only in Dataset 1: {merge_stats.get('left_only', 0)}\")\n",
        "print(f\"  • Records only in Dataset 2: {merge_stats.get('right_only', 0)}\")\n",
        "\n",
        "# Drop merge indicator\n",
        "df_merged = df_merged.drop('_merge', axis=1)\n",
        "\n",
        "# Display sample of merged data\n",
        "print(\"\\nSample of merged dataset:\")\n",
        "display(df_merged[['VIN', 'TRANSACTION_ID', 'CAUSAL_PART_NM', 'TOTALCOST', 'CAUSAL_VERBATIM_COMBINED']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Join Type Justification\n",
        "\n",
        "**SELECTED JOIN TYPE:** `LEFT JOIN`\n",
        "\n",
        "**JUSTIFICATION:**\n",
        "1. **Preserve Main Data:** All repair transaction records from Dataset 1 must be retained for complete analysis\n",
        "2. **Business Priority:** The main repair data is the primary source of truth\n",
        "3. **Data Completeness:** Ensures no repair records are lost in the integration\n",
        "\n",
        "**ALTERNATIVE JOIN TYPES - IMPLICATIONS:**\n",
        "\n",
        "**INNER JOIN:**\n",
        "- Would keep only records with matching TRANSACTION_IDs in both datasets\n",
        "- Risk: Loss of repair records without causal verbatims\n",
        "- Impact: Reduced dataset size, potentially biased analysis\n",
        "\n",
        "**RIGHT JOIN:**\n",
        "- Would keep all causal verbatims, only matching repair records\n",
        "- Risk: Orphan causal verbatims without repair context\n",
        "- Impact: Meaningless records without repair metadata\n",
        "\n",
        "**OUTER JOIN:**\n",
        "- Would keep all records from both datasets\n",
        "- Risk: Records without proper context from either side\n",
        "- Impact: Larger dataset but with incomplete records"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Save Merged Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save merged dataset\n",
        "print(\"\\nSaving merged dataset...\")\n",
        "df_merged.to_csv('task2_merged_dataset.csv', index=False)\n",
        "df_merged.to_excel('task2_merged_dataset.xlsx', index=False)\n",
        "print(\"  ✓ Saved: task2_merged_dataset.csv\")\n",
        "print(\"  ✓ Saved: task2_merged_dataset.xlsx\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TASK 2 COMPLETED!\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# TASK 3: EXPLORATORY DATA ANALYSIS\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Trend Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"SECTION 1: TREND ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization 1: Repair Cost vs Vehicle Age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n1. Creating Visualization: Repair Cost vs Vehicle Age...\\n\")\n",
        "\n",
        "# Filter valid data\n",
        "cost_age_data = df_merged[(df_merged['TOTALCOST'] > 0) & \n",
        "                   (df_merged['REPAIR_AGE'].notna()) & \n",
        "                   (df_merged['REPAIR_AGE'] >= 0) &\n",
        "                   (df_merged['REPAIR_AGE'] < 100)].copy()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Scatter plot\n",
        "axes[0].scatter(cost_age_data['REPAIR_AGE'], cost_age_data['TOTALCOST'], \n",
        "               alpha=0.5, c='steelblue', edgecolors='black', s=50)\n",
        "axes[0].set_xlabel('Vehicle Age at Repair (months)', fontsize=12)\n",
        "axes[0].set_ylabel('Total Repair Cost ($)', fontsize=12)\n",
        "axes[0].set_title('Repair Cost vs Vehicle Age', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Calculate and plot trend line\n",
        "z = np.polyfit(cost_age_data['REPAIR_AGE'], cost_age_data['TOTALCOST'], 1)\n",
        "p = np.poly1d(z)\n",
        "axes[0].plot(cost_age_data['REPAIR_AGE'], p(cost_age_data['REPAIR_AGE']), \n",
        "            \"r--\", linewidth=2, label='Trend')\n",
        "axes[0].legend()\n",
        "\n",
        "# Box plot by age groups\n",
        "cost_age_data['AGE_GROUP'] = pd.cut(cost_age_data['REPAIR_AGE'], \n",
        "                                     bins=[0, 12, 24, 36, 100],\n",
        "                                     labels=['0-12m', '12-24m', '24-36m', '36m+'])\n",
        "\n",
        "cost_age_data.boxplot(column='TOTALCOST', by='AGE_GROUP', ax=axes[1])\n",
        "axes[1].set_xlabel('Vehicle Age Group', fontsize=12)\n",
        "axes[1].set_ylabel('Total Repair Cost ($)', fontsize=12)\n",
        "axes[1].set_title('Cost Distribution by Age Group', fontsize=14, fontweight='bold')\n",
        "plt.suptitle('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('eda_viz1_cost_vs_age.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate correlation\n",
        "correlation = cost_age_data['REPAIR_AGE'].corr(cost_age_data['TOTALCOST'])\n",
        "print(f\"✓ Saved: eda_viz1_cost_vs_age.png\")\n",
        "print(f\"  Correlation coefficient (Age vs Cost): {correlation:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization 2: Component Failure Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n2. Creating Visualization: Failure Component Analysis Heatmap...\\n\")\n",
        "\n",
        "# Analyze relationship between parts and platforms\n",
        "if 'CAUSAL_PART_NM' in df_merged.columns and 'PLATFORM' in df_merged.columns:\n",
        "    # Get top components and platforms\n",
        "    top_parts = df_merged['CAUSAL_PART_NM'].value_counts().head(10).index\n",
        "    top_platforms = df_merged['PLATFORM'].value_counts().head(6).index\n",
        "    \n",
        "    # Create crosstab\n",
        "    heatmap_data = pd.crosstab(\n",
        "        df_merged[df_merged['CAUSAL_PART_NM'].isin(top_parts)]['CAUSAL_PART_NM'],\n",
        "        df_merged[df_merged['PLATFORM'].isin(top_platforms)]['PLATFORM']\n",
        "    )\n",
        "    \n",
        "    plt.figure(figsize=(14, 8))\n",
        "    sns.heatmap(heatmap_data, annot=True, fmt='d', cmap='YlOrRd', \n",
        "               cbar_kws={'label': 'Number of Failures'}, linewidths=0.5)\n",
        "    plt.title('Component Failure Frequency by Vehicle Platform', \n",
        "             fontsize=14, fontweight='bold', pad=20)\n",
        "    plt.xlabel('Vehicle Platform', fontsize=12)\n",
        "    plt.ylabel('Failed Component', fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('eda_viz2_component_platform_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"✓ Saved: eda_viz2_component_platform_heatmap.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization 3: Temporal Trends"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n3. Creating Visualization: Repair Trends Over Time...\\n\")\n",
        "\n",
        "if 'REPAIR_DATE' in df_merged.columns:\n",
        "    # Ensure date is datetime\n",
        "    df_merged['REPAIR_DATE'] = pd.to_datetime(df_merged['REPAIR_DATE'], errors='coerce')\n",
        "    \n",
        "    # Filter valid dates\n",
        "    date_data = df_merged[df_merged['REPAIR_DATE'].notna()].copy()\n",
        "    date_data['YEAR_MONTH'] = date_data['REPAIR_DATE'].dt.to_period('M')\n",
        "    \n",
        "    # Count repairs per month\n",
        "    monthly_repairs = date_data.groupby('YEAR_MONTH').size()\n",
        "    \n",
        "    # Average cost per month\n",
        "    monthly_cost = date_data.groupby('YEAR_MONTH')['TOTALCOST'].mean()\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
        "    \n",
        "    # Plot 1: Repair frequency\n",
        "    monthly_repairs.plot(ax=axes[0], color='steelblue', linewidth=2, marker='o')\n",
        "    axes[0].set_title('Repair Frequency Over Time', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('Month', fontsize=12)\n",
        "    axes[0].set_ylabel('Number of Repairs', fontsize=12)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Average cost trend\n",
        "    monthly_cost.plot(ax=axes[1], color='coral', linewidth=2, marker='s')\n",
        "    axes[1].set_title('Average Repair Cost Over Time', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_xlabel('Month', fontsize=12)\n",
        "    axes[1].set_ylabel('Average Cost ($)', fontsize=12)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('eda_viz3_temporal_trends.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"✓ Saved: eda_viz3_temporal_trends.png\")\n",
        "\n",
        "print(\"\\n✓ All trend visualizations created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Root Cause Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Failure Components Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"SECTION 2: ROOT CAUSE IDENTIFICATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n1. FAILURE COMPONENT ANALYSIS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "if 'CAUSAL_PART_NM' in df_merged.columns:\n",
        "    top_failures = df_merged['CAUSAL_PART_NM'].value_counts().head(10)\n",
        "    \n",
        "    print(\"\\nTop 10 Failing Components:\")\n",
        "    for i, (component, count) in enumerate(top_failures.items(), 1):\n",
        "        percentage = (count / len(df_merged)) * 100\n",
        "        print(f\"  {i}. {component}\")\n",
        "        print(f\"     Failures: {count} ({percentage:.1f}% of all repairs)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate cost impact by component\n",
        "print(\"\\n\\nCost Impact by Component (Top 5):\")\n",
        "for component in top_failures.head(5).index:\n",
        "    component_data = df_merged[df_merged['CAUSAL_PART_NM'] == component]\n",
        "    avg_cost = component_data['TOTALCOST'].mean()\n",
        "    total_cost = component_data['TOTALCOST'].sum()\n",
        "    print(f\"\\n  {component}:\")\n",
        "    print(f\"    • Average cost per repair: ${avg_cost:.2f}\")\n",
        "    print(f\"    • Total cost impact: ${total_cost:.2f}\")\n",
        "    print(f\"    • Number of incidents: {len(component_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Failure Conditions Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\\n2. FAILURE CONDITION PATTERNS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Analyze customer verbatims for common failure patterns\n",
        "if 'CUSTOMER_VERBATIM' in df_merged.columns:\n",
        "    failure_patterns = {\n",
        "        'Not Working/Inoperative': ['inop', 'not working', \"doesn't work\", \"won't work\"],\n",
        "        'Physical Damage': ['coming apart', 'cracked', 'broken', 'damaged', 'peeling'],\n",
        "        'Intermittent Issue': ['intermittent', 'sometimes', 'occasionally'],\n",
        "        'Warning Messages': ['message', 'warning', 'light on', 'code'],\n",
        "        'Noise/Sound': ['noise', 'rattle', 'squeak', 'sound']\n",
        "    }\n",
        "    \n",
        "    pattern_counts = {}\n",
        "    for pattern, keywords in failure_patterns.items():\n",
        "        count = 0\n",
        "        for keyword in keywords:\n",
        "            count += df_merged['CUSTOMER_VERBATIM'].str.contains(keyword, case=False, na=False).sum()\n",
        "        pattern_counts[pattern] = count\n",
        "    \n",
        "    print(\"\\nCommon Failure Conditions:\")\n",
        "    for pattern, count in sorted(pattern_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "        percentage = (count / len(df_merged)) * 100\n",
        "        print(f\"  • {pattern}: {count} occurrences ({percentage:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Repair Type Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\\n3. REPAIR/FIX TYPE ANALYSIS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "if 'GLOBAL_LABOR_CODE_DESCRIPTION' in df_merged.columns:\n",
        "    top_repairs = df_merged['GLOBAL_LABOR_CODE_DESCRIPTION'].value_counts().head(8)\n",
        "    \n",
        "    print(\"\\nMost Common Repair Types:\")\n",
        "    for i, (repair, count) in enumerate(top_repairs.items(), 1):\n",
        "        percentage = (count / len(df_merged)) * 100\n",
        "        print(f\"  {i}. {repair}\")\n",
        "        print(f\"     Frequency: {count} ({percentage:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Root Cause Synthesis and Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ROOT CAUSE SYNTHESIS FOR STAKEHOLDERS\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\"\"\n",
        "KEY FINDINGS:\n",
        "\n",
        "1. DOMINANT FAILURE MODES:\n",
        "   • Steering wheel components show highest failure rates\n",
        "   • Physical wear and tear (peeling, delaminating) is common\n",
        "   • Electrical/heating components have significant failure rates\n",
        "\n",
        "2. COST DRIVERS:\n",
        "   • Complete steering wheel replacements are most expensive\n",
        "   • Module replacements represent moderate cost impact\n",
        "   • Labor costs constitute significant portion of total repair cost\n",
        "\n",
        "3. FAILURE TIMING:\n",
        "   • Early failures (< 12 months) indicate potential manufacturing defects\n",
        "   • Mid-life failures (12-36 months) suggest design/quality issues\n",
        "   • Late failures (36+ months) align with normal wear expectations\n",
        "\n",
        "4. PLATFORM-SPECIFIC ISSUES:\n",
        "   • Full-Size Trucks show highest failure volumes\n",
        "   • Specific platforms may have design vulnerabilities\n",
        "   • Consistency across platforms suggests supplier/process issues\n",
        "\n",
        "RECOMMENDATIONS FOR STAKEHOLDERS:\n",
        "\n",
        "IMMEDIATE ACTIONS:\n",
        "  1. Conduct root cause analysis on top 3 failing components\n",
        "  2. Review manufacturing processes for steering wheel assembly\n",
        "  3. Evaluate supplier quality for high-failure parts\n",
        "  4. Implement enhanced quality checks for early-life failures\n",
        "\n",
        "MEDIUM-TERM IMPROVEMENTS:\n",
        "  5. Redesign components with high failure rates\n",
        "  6. Enhance warranty coverage for identified failure modes\n",
        "  7. Develop predictive maintenance alerts for at-risk components\n",
        "  8. Improve technician training for complex repairs\n",
        "\n",
        "LONG-TERM STRATEGY:\n",
        "  9. Implement advanced quality control using AI/ML\n",
        "  10. Establish continuous monitoring of field failure data\n",
        "  11. Integrate customer feedback into design processes\n",
        "  12. Develop proactive recall strategy for systemic issues\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TASKS 2 & 3 COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nGenerated Files:\")\n",
        "print(\"  1. task2_merged_dataset.csv\")\n",
        "print(\"  2. task2_merged_dataset.xlsx\")\n",
        "print(\"  3. eda_viz1_cost_vs_age.png\")\n",
        "print(\"  4. eda_viz2_component_platform_heatmap.png\")\n",
        "print(\"  5. eda_viz3_temporal_trends.png\")\n",
        "print(\"\\nAll analyses completed. Review files and console output for insights.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
